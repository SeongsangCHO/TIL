###### 1. 학습 날짜

- 2020 - 11 - 12(목)

---

###### 2. 학습시간

- 10:00 ~ 18:00

---

###### 3. 학습 범위 및 주제

- 쿠버네티스 정리 

---

###### 4. 동료 학습 방법 

- daelee, taelee, jehong, mihykim

---

###### 5. 학습 목표 

- 쿠버네티스를 정리한다.

---

###### 6. 상세 학습 내용

- 코드작성시간 :  시간

# service

> 출처는 하단에 남겨놓았습니다.
>
> [내용의 전반적인 내용은 해당링크를 대부분 참고](https://subicura.com/2019/05/19/kubernetes-basic-1.html#%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EB%9E%80)



설정에 필요한 파일들은 `srcs`에 있어야함

`setup.sh`파일은 레포지토리의 최상단에 위치해야하며, 이 스크립트는 내 애플리케이션의 모든 세팅을 해야함

도커와 쿠버네티스에 대해 쫄지 말것..



쿠버네티스를 사용해서 각기 다른 서비스들의 기반구조를 세팅해야함

`multi-service cluster`으로 셋업해야함

**각 서비스는 컨테이너 안에서 수행되야함**

각 컨테이너는 `Alpine Linux` 로 빌드되고 관련 서비스와 성능을 위해 동일한 이름을 가져야한다.  

또한, 컨테이너들은 `setup.sh`에서 작성된 도커파일을 가져야한다.

도커허브와같이 빌드된 이미지들을 사용하는건 금지한다. 직접 이미지를 빌드하고 사용할 수 있어야한다.



## 셋업사항

- 클러스터를 관리하기위한 쿠버네티스 `Web dashboard`
- 서비스들에 대한 외부 접근을 관리하는 `Load Balancer`. 이는 오직 클러스터의 진입포인트에 위치해야한다. 3000번포트를 사용하고 `Load Balancer` 은 단일 IP를 가져야한다.
- 5050포트를 이용하는 `WordPress` 웹사이트, `MySql`데이터베이스를 포함하고 있어야하며 이 둘의 서비스는 각기 분리된 컨테이너에서 동작해야한다. `WordPress Website`  는 여러 유저와 하나의 관리자를 가져야하고 자신의 nginx server를 가져야한다. `Load Balancer` 은 이 서비스로 직접 리디렉트할 수 있어야한다.
- 5000번포트를 이용하는 `phpMyAdmin`. `Mysql` 과 연동되어 있어야하고, 자신만의 nignx Server를 가져야함. `Load Balancer` 가 이 서비스로 리디렉트할 수 있어야함.
- 컨테이너는 80, 443포트를 이용한다. 80포트는 http이고, 301에서 443으로 리디렉션해야함 (http to https), `/wordpress` 경로로 접근하면 `307` 리디렉트를 `IP:WPPORT` 로..한다..? `/phpmyadmin` 은 `reverse proxy to IP:PMAPORT` 허용한다.
  - /wordpress, /phpmyadmin요청은 `IP: ~~~` 로 리디렉션하라는 뜻인거 같음? ip/wordpress가 아닌 phpip로 접속되게?
- `FTPS` 서버는 21번포트를 사용한다.
- `Grafana` 플랫폼은 3000번포트를 이용하고 `Influx DB`에 연결되야함 이는 나의 모든 컨테이너들을 모니터링할 수 있는 것. 하나의 서비스마다 하나의 대시보드를 만들어라. `Influx DB` 와  `Grafana` 는 다른 컨테이너에 있어야한다.
- 둘중 하나의 DB컨테이너에서 충돌이 난 경우에는  DB가 지속되는지 확인해야한다.
- nginx컨테이너에 SSH에 로그인을 함으로써 접근할 수 있다.
- 하나 또는 두개의 컴포턴트 부분들에서 충돌 또는 멈출경우 모든 컨테이너는 재시작해야한다.
- 서비스에 대해 각 리디렉션 로드밸런스를 사용해 수행되는지 확인, `FTPS, Grafana, Wordpress, Phpmyadmin, nginx` 로드밸런스해주어야함 MySQL는 ClusterIP여야함. NodePort가 될 수 없음.







# 쿠버네티스란?

> k8s라고도 불리우며, kube라고도 한다.
>
> **컨테이너를 쉽고, 빠르게 배포/확장하며 컨테이너의 관리를 자동화** 해주는 오픈소스 플랫폼



## Why?

왜 쿠버네티스?

쿠버네티스의 [공식페이지](https://kubernetes.io/ko/docs/concepts/overview/what-is-kubernetes/#:~:text=%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EB%8A%94%20%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88%ED%99%94,%EC%9E%90%EB%8F%99%ED%99%94%EB%A5%BC%20%EB%AA%A8%EB%91%90%20%EC%A7%80%EC%9B%90%ED%95%9C%EB%8B%A4.)에 따르면 배포시대는 총 3개로 나뉜다.

- 전통적인 배포시대 : 애플리케이션을 물리서버에 실행하는 방식. 한 물리서버에 여러 애플리케이션의 리소스한계를 정의할 수 없어서 리소스 할당 문제가 발생. => 비용이 많이듦

- 가상화된 배포시대 : 위의 해결책으로 **가상화** 도입. 단일 물리서버의 CPU에서 여러 가상시스템을 실행함. 가상화를 사용하면 VM간의 애플리케이션을 격리하고 앱끼리 액세스가 자유롭지 못하므로 일정수준 보안성을 제공함.

  - 가상화 : 하이퍼바이저라는 SW로 물리적 머신 위에 다수의 VM을 만듦, 단일 서버에서 여러 OS를 실행할 수 있음. 가상화를 사용하지 않으면 서버 한대당 하나의 기능만 수행할 수 있어 리소스가 남으면 이를 사용할 수 없음, 남는 서버공간을 활용할 수 없다 . 유지비용이 ++

  

- 컨테이너 개발시대  :  컨테이너는 VM과 유사, 그러나 격리속성을 완화하여 **앱간 OS를 공유함** 

  - 컨테이너는 리눅스기반에서 프로세스간 격리를 위해 사용된 기술들을 조합해 발전시킨 것으로 **호스트 OS 리소스를 공유하고, 호스트 OS위에서 애플리케이션을 작동시키기 위해 필요한 라이브러리 등을 모아서 별도의 서버인 것처럼 사용. 호스트의 OS커널을 공유해 OS환경을 그대로 따라야함 **

  - Chroot, namespace, cgroup를 조합.

  - chroot : 특정 디렉토리를 루트로 인식하게 끔 하는 명령어

  - Namespace: 리눅스 시스템 자원을 묶어서 ps에 할당. 하나의 ps 자원을 관리하는 기능

  - cgroup : cpu, 메모리 등 ps그룹의 자원 사용량을 관리해 특정 앱 자원과다사용을 제한

  - chroot를 통해 특정 파일 디렉토리를 루트로 인식되도록하고, namespace, cgroup로 특정 ps에게 자원 할당, 제어하는 방식 -> 이렇게 설정된 디렉토리는 jail이라고도 하는데 해당 디렉토리에서는 다른 자원에 접근할 수 없음 

  - 이런 격리된 공간은 아직 완벽한 가상환경이 아니었음. 그래서 LXC (Linux Container)이 등장. 커널레벨에서 제공되는 격리된 가상공간, (**OS 자체**를 가상화하지 않아 컨테이너라고 부름)

    



더 많은 컨테이너들이 생성되고 사용하면서 여러 컨테이너들을 관리해야하는 방법이 필요로 해졌다.  배포환경에서 컨테이너가 다운되었을 때 다시 이 컨테이너를 시작해주어야한다. 그런데 이를 시스템이 처리해준다면?

그걸 쿠버네티스가 해준다.

이런 분산 시스템을 실행하기 위한 프레임워크를 제공, 로드밸런싱, 자동복구.. 등등을 쿠버네티스가 제공해준다.













> 한마디 정리
>
> **가상머신은 운영체제 위에 하드웨어를 에뮬레이션하고 그 위에 운영체제를 올리고 프로세스를 실행하는 반면에, 도커 컨테이너는 하드웨어 에뮬레이션 없이 리눅스 커널을 공유해서 바로 프로세스를 실행한다**
>
> VM이 Container보다 더 강력히 격리
>
> VM은 따로 OS가 올라가서 커널을 공유하지 않아 거의 완벽히 host와 분리
>
> VM은 io발생시 host와 분리되어서 host os에 맞게 처리해줘야하므로 느림
>
> Container은 OS부분을 가상화하고 커널을 host와 공유
>
> Container은 커널을 공유해서 io처리가 빠름



## 개념 이해

쿠버네티스를 이해하기 위해 꼭 이해하고 넘어가자

### 클러스터

>  컨테이너화된 앱을 실행하기 위한 노드 머신
>
>  하나의 노드는 여러개의 파드들을 가질 수 있고 마스터는 노드를 통해 파드들에 대한 스케쥴링을 자동으로 처리한다.
>
>  큐블릿은 마스터-노드간 통신을 책임지는 프로세스이고 하나의 머신 상에서 동작하는 파드와 컨테이너를 관리
>
>  

클러스터 전체를 관리하는 `master`, 컨테이너를 배포하는 머신인 `node`로 이루어져있다.

![](https://images.velog.io/images/secho/post/5327776c-62b6-4089-9d5b-0b84fb2cf136/%E1%84%8F%E1%85%B3%E1%86%AF%E1%84%85%E1%85%A5%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5.png)



### 오브젝트

컨테이너화되어 배포되는 앱의 워크로드를 기술함 Pod, Service, Volume, Namespace 4가지가 있음



### Pod

>  쿠버네티스에서 컨테이너를 포함한 가장 기본적인 배포 단위
>
>  쿠버네티스에서의 워커머신인 노드상에서만 동작하고, 노드는 클러스터에 따라 가상 또는 물리머신일 수 있다.

쿠버네티스는 하나의 컨테이너를 개별적으로 배포하는것이 아니라 Pod 단위로 배포하는데, 

이 Pod은 하나 이상의 컨테이너를 포함한다. 

즉, 쿠버네티스는 하나 이상의 컨테이너를 Pod단위로 배포하는 것

```php
apiVersion: v1 					//해당 스크립트를 실행하기 위한 쿠버네티스 api버전, 보통 v1
kind: Pod 							// 리소스의 종류를 정의
metadata: 							// 각종 메타데이터, 라벨, 리소스의 이름 등을 넣음
	name: nginx 
spec: 									//리소스에 대한 상세한 스펙 정의
	containers: 					// Pod는 컨테이너를 가지고 있어서 이를 정의
	- name: nginx 				// 컨테이너 이름은 nginx 
		image: nginx:1.7.9 // 도커 이미지 nginx 1.7.9 버전 사용
		ports: 						// 컨테이너 포트는 8090
		- containerPort: 8090
```

하나이상의 컨테이너를 가진 Pod은 다음과 같은 특징을 가진다.

- Pod내의 컨테이너 끼리 IP, Port를 공유한다.
- Pod내 컨테이너는 localhost를 통해 통신할 수 있다. (A Pod내 1, 2 컨테이너끼리 localhost:xxx, localhost:ccc로 통신 가능)
- 컨테이너 간 디스크 볼륨을 공유할 수 있음 ( 다른 컨테이너 파일을 읽어올 수 있다)



메타데이터 : 대량의 정보중에 찾는 정보를 효율적으로 찾기 위해 규칙에 따라 콘텐츠에 대해 부여되는 데이터 ( HTML의 table-tr-td처럼  tree구조로 규칙을 정해놓음)

### Service

Pod을 서비스할 때 하나를 제공하는 경우는 드뭄, 따라서 여러개 Pod을 서비스하게 되는데 이때 로드밸런서를 이용해 하나의 IP, Port를 묶어 제공한다.

Pod은 동적으로 생성, 장애발생시 자동으로 리스타트되면서 **IP가 바뀌게** 되는데 로드밸런서가 이 바뀌는 IP를 지정하는것은 어렵기 때문에 로드밸런서가 동적으로 변화하는 pod들의 목록을 선택하기 위해 **라벨, 라벨셀렉터** 를 사용한다.



서비스는 라벨셀렉터에서 특정 라벨을 가진 Pod만을 선택해 서비스로 묶어 그 Pod간에만 로드밸런싱을 통해 외부로 서비스를 제공함

![](https://images.velog.io/images/secho/post/a02e11d6-1ac9-4bac-956c-5d03eac596a0/%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3.png)

```php
kind: Service					//리소스 종류 : Service
apiVersion: v1				
metadata:
	name: my-service
spec:
	selector:
		app: myapp			//라벨이 app:myapp인 Pod만 선택해 서비스를 제공하도록
	ports:						//port는 80, tcp프로토콜
	- protocol: TCP
	port: 80				//80port의 요청을 컨테이너의 9376포트로 연결.
	targetPort: 9376
```

- myapp이라고 불리는 pod을 선택해 80으로 요청이 들어오면 9376으로 연결하도록 하는것?



### 아키텍쳐

![](https://images.velog.io/images/secho/post/b6aa3caa-72f3-44e1-9af6-06e1dd76b551/%E1%84%80%E1%85%AE%E1%84%8C%E1%85%A9.png)

쿠버네티스는 마스터, 노드 구조로 구성되어있다.

모든 명령은 마스터의 API서버를 호출하고 노드는 마스터와 통신하면서 작업을 수행한다.

노드는 마스터와 통신하며 필요한 Pod를 생성하고 네트워크를 설정한다. 라벨을 붙여 사용 목적을 정의한다.

Kubectl : 명령행 도구. => 마스터의 API서버는 json등의 형식을 이용한 http통신을 지원하는데 이는 불편해서 `kubectl` 을 사용한다.



#### API서버

![](https://images.velog.io/images/secho/post/8c251cbb-e3b2-46e6-bbd9-0eab50190d25/%E1%84%86%E1%85%A1%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5.png)

마스터는 API서버, etcd, 스케쥴러, 큐브, 클라우드 컨트롤러로 구성되어있다.

- etcd: key-value저장소 설정, 상태데이터를 저장, 나머지 모듈들은 `stateless`하게 동작해서 etcd만 잘 백업하면 언제든지 클러스터(마스터, 노드)를 복구할 수 있음
- 스케쥴러 : 할당되지 않은 Pod을 자원, 라벨에 따라 적절한 노드 서버로 할당
- 큐브 컨트롤러 : 거의 모든 오브젝트의 상태를 관리함. deployment는 replicaset을 생성, ReplicaSet은 Pod를 생성, Pod는 스케쥴러가 관리

![](https://images.velog.io/images/secho/post/465ddc6a-db23-423e-aeae-d019587bf4ab/%E1%84%82%E1%85%A9%E1%84%83%E1%85%B3.png)

Kubelet(큐블릿)은 Pod 생명주기를 관리- pod안의 컨테이너에 이상이 있는지 체크하고 주기적으로 마스터에 상태를 전달

프록시 : pod으로 연결되는 네트워크 관리. 프록시를 프록시서버로 사용했는데 iptables, IPVS방식으로 지원하기 시작





## Pod생성과정



![](https://images.velog.io/images/secho/post/1c1c50dd-d18f-46bd-8ba3-cb7c90b6c08d/%E1%84%91%E1%85%A1%E1%86%BA%20%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC.png)



> 레플리카셋: 지정한 파드의 갯수만큼 항상 실행할 수 있도록 관리해주는 것.
>
> 5개로 지정해두면 1개가 삭제되었을 때 1개 파드를 생성해 5개를 유지하도록 함 

모듈끼리 통신하지 않고 api서버와만 통신.

api서버를 통해 etcd에 있는 상태를 체크해 현재 상태와 원하는 상태가 다르면 필요한 작업을 수행한다.



#### 각 모듈이 하는일



#### kubectl - 관리파일을 정의하고 api서버에 전달하는 역할

- 레플리카셋 명세를 yml파일로 정의, kubectl도구로 api 서버에 명령전달
- Api server는 새로운 레플리카 오브젝트를 etcd에 저장

#### kube controller - 레플리카셋을 직접 관리, 파드 생성해주는 부분

- 큡컨트롤러의 레플리카셋 컨트롤러가 레플리카셋을 감시하다가 레플리카셋에 정의된 라벨셀렉터 조건을 만족하는 파드가 있는지 체크

- 해당하는 라벨의 파드가 없으면 레플리카셋의 파드 템플릿을 보고 새로운 파드를 생성. 이때 생성은 api서버에 전달하고 서버는 etcd에 저장

  > 레플리카컨트롤러가 라벨셀렉터에 만족하는 파드를 체크하다가 해당 파드가 없으면 새로운 파드를 api서버와 통신하면서 생성하는 것

#### Scheduler 비할당된 파드 관리

- 할당되지 않은 파드가 있는지 체크. 그런 파드가 있으면 조건에 맞는 노드를 찾아 해당 파드 할당

#### Kubelet 생성되지 않은 파드 체크, 파드상태 전달

- 자신과 같은 노드에 할당되었지만, 생성되지 않은 파드가 있는지 체크
- 생성되지 않은 파드가 있으면 명세를 보고서 파드 생성
- 파드의 상태를 주기적으로 api서버에 전달



## kubectl 명령어

> Kubectl -> k로 통칭하겠다

- k get - 자원을 나열
  - k get pods 파드들 나열
- k describe - 자원에 대한 상세정보
  - k describe pods - 파드 컨테이너에 대한 세부 정보(ip, port, 수명주기 등등)
- k logs 파드name - 파드 내 컨테이너의 로그들을 출력
- k exec - 파드 내 컨테이너에 대한 명령 실행
  - k exec $pod_name env - 파드 환경변수 확인

### Deployment (Kubectl로 생성, 관리)

> 레플리카셋의 상위 개념으로 배포작업을 좀 더 세분화할 수 있는 기능을 가짐.
>
> 쿠버네티스가 애플리케이션의 인스턴스를 어떻게 생성, 업데이트해야 하는지를 지시

클러스터를 구동시키면, 그 위에 컨테이너화된 애플리케이션을 배포할 수 있는데 그러기 위해서 **디플로이먼트 설정** 을 만들어야 함.

디플로이먼트 컨트롤러는 애플리케이션 인스턴스를 모니터링하고, 이를 구동 중인 노드에 문제가 생기면 다른 노드의 인스턴스로 교체한다. 이를 **자동 복구 메커니즘** 이라한다.



#### 디플로이먼트 생성

```shell
kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 # 디플로이먼트 생성

kubectl get deployments # 디플로이먼트 목록 확인 
```



#### 외부에 애플리케이션 노출(프록시 설정)

> kubectl proxy

쿠버네티스 내의 파드들은 격리된 비공개 네트워크안에 있어서 외부에서 볼 수 없다.

kubectl명령어로 통신을 클러스터 전체의 사설 네트워크로 전달하는 프록시를 만들 수 있다.

먼저 파드의 이름을 얻어야 함.







### 서비스, 레이블, 오브젝트

> 서비스를 이용해 클러스터 외부로 애플리케이션을 노출한다.

파드들은 생명주기를 갖고 워커노드가 죽으면 해당 파드들은 죽거나 종료된다.

위에서 작성한 `레플리카셋` 은 미리 지정해둔 상태로 파드들을 동적으로 생성할 수 있다,

각 파드들은 고유의 IP를 갖고 있지만 서비스 도움없이 외부로 노출될 수 없다.

서비스는 type을 지정함으로써 다양한 방법으로 노출할 수 있다. 우리는 여기서 로드밸런서를 이용할 것



- *ClusterIP* (기본값) - 클러스터 내에서 내부 IP 에 대해 서비스를 노출해준다. 이 방식은 오직 클러스터 내에서만 서비스가 접근될 수 있도록 해준다.
- *NodePort* - NAT가 이용되는 클러스터 내에서 각각 선택된 노드들의 동일한 포트에 서비스를 노출시켜준다. `<NodeIP>:<NodePort>`를 이용하여 클러스터 외부로부터 서비스가 접근할 수 있도록 해준다. ClusterIP의 상위 집합이다.
- *LoadBalancer* - (지원 가능한 경우) 기존 클라우드에서 외부용 로드밸런서를 생성하고 서비스에 고정된 공인 IP를 할당해준다. NodePort의 상위 집합이다.
- *ExternalName* - 이름으로 CNAME 레코드를 반환함으로써 임의의 이름(스펙에서 `externalName`으로 명시)을 이용하여 서비스를 노출시켜준다. 프록시는 사용되지 않는다. 이 방식은 `kube-dns` 버전 1.7 이상에서 지원 가능하다.



# 설치

로드밸런서 설정을 마무리하고 [외부에서 접속하기](https://kubernetes.io/ko/docs/tutorials/stateless-application/expose-external-ip-address/) 예제를 따라하면 `EXTERNAL-IP` 가 계속 pending상태인데 이는 [로드밸런서 타입](https://bcho.tistory.com/1308) 여기서 간단하게 해결 가능.





# 설치

minikube를 설치하기위해 `brew` 를 먼저 설치해야함

> curl -fsSL https://rawgit.com/gcamerli/42brew/master/set.sh | zsh
>
> brew install minikube
>
> minikube start --driver=virtualbox
>
> 여기까지 설치하고 goinfre로 파일을 옮겨 링크를 생성하자
>
> mv .minikube goinfre/minikube
>
> ln -s goinfre/minikube/ .minikube
>
> minikube start --driver=virtualbox





출처

- [컨테이너와 가상머신의 차이](https://www.44bits.io/ko/post/is-docker-container-a-virtual-machine-or-a-process)

- [쿠버네티스 개념이해](https://bcho.tistory.com/1256#:~:text=%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9D%98%20%ED%8A%B9%EC%A7%95,%EC%9D%B4%EC%83%81%EC%9D%98%20%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88%EB%A5%BC%20%ED%8F%AC%ED%95%A8%ED%95%9C%EB%8B%A4.&text=apiVersion%EC%9D%80%20%EC%9D%B4%20%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8%EB%A5%BC,%EB%B3%B4%ED%86%B5%20v1%EC%9D%84%20%EC%82%AC%EC%9A%A9%ED%95%9C%EB%8B%A4.)

  


---

###### 7. 학습 내용에 대한 개인적인 총평

- 내용이 생각보다 많다.
- 컨테이너와 VM의 차이점을 잘 정리해둘것

###### 8. 다음 학습 계획

- 레이아웃 작성