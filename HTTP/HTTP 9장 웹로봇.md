## HTTP 9장 웹로봇





<br>

### 웹로봇이란

웹로봇은 사람과 상호작용 없이 웹 트랜잭션을 자동으로 수행하는 SW이다.

크롤러, 웜, 봇 등이 존재한다.

<br>

### 9.1 크롤러와 크롤링

웹 크롤러는 웹 페이지를 가져오는 행위를 재귀적으로 반복하는 로봇이다.

크롤러라는 이름이 붙은 이유는 HTML 하이퍼링크들로 만들어진 웹을 기어다니기(crawl) 때문이다.



#### 크롤러의 시작.

크롤러에게 초기 시작지점을 알려주어야한다. 이 시작지점을 **루트집합**이다.

웹은 연결되어있지만 고립된 페이지도 존재한다. 모든 페이지에 접근하기 위해 이 루트집합이 필요하다.

크롤러는 크롤링할 페이지들을 찾아야한다. HTML을 파싱하여 링크를 추출해서 절대 링크로 변환해야한다.

크롤링할 때 무한루프에 빠지지 않도록 주의해야한다. 예를들어 A페이지 방문 후 B로, 다시 B에서 A로 가는 상황을 막아야한다.

이 때문에 어디에 방문했는지를 알아야한다.

방문한 URL을 추적하기 위해 **검색트리, 해시테이블**을 사용할 수 있다.

대규모의 크롤러는 **존재 비트 배열**이라는 자료구조로 방문페이지를 체크한다.



<br>



#### 9.1.1대규모의 웹크롤러의 방문페이지 관리 기법

<br>

##### 트리, 해시테이블

- 방문한 URL을 추적하기 위해 효율적인 검색트리, 해시테이블 자료구조를 사용한다.

##### 느슨한 존재 비트맵

- 대규모 크롤러들은 **존재 비트 배열**이라는 자료구조를 사용한다.
- 각 URL은 해시함수에 의해 고정된 크기의 숫자로 변환되어 배열안에 대응하는 존재 비트를 가지며, 크롤링되었을 때 해당하는 존재 비트가 생성됨. 이 존재비트로 크롤링의 유무를 판단할 수 있다.

- 그러나 URL갯수는 잠재적으로 무한, 존재 비트배열에는 유한한 비트만이 존재한다. 따라서 같은 존재비트에 두 URL이 매핑되어 충돌할 수 있다.





##### 체크포인트

- 로봇의 중단에 대비해 방문 URL목록이 저장되있는지 확인.

<br>



##### 파티셔닝

- 웹이 커지면서 한대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것이 불가능하기 때문에 URL의 특정 부분에 할당되어 파티셔닝한다.

<br>



#### 9.1.2 URL 정규화하기

웹 로봇은 URL을 정규화함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것을 미리 제거하려한다.

다음 방식으로 로봇은 URL을 정규화한다.

1. 포트번호가 없으면 :80을 추가
2. 모든 %xx 이스케이핑문자를 대응되는 문자로 변환
3. #태그제거



##### 루프 피하기

악의적인 웹 마스터들을 심볼릭링크를 사용해 크롤러들이 무한루프에 빠지도록 설계하기도 한다.

모든 루프를 피할 수 없지만, 피하기 위해 휴리스틱 집합을 필요로 한다고한다.

하지만 이도 약간의 손실을 유발시킨다.

웹에서 로봇이 더 올바르게 동작하기 위해 다음과 같은 기법들을 사용한다.



- URL정규화 : URL을 표준형태로 변환
- 너비 우선 크롤링 : 웹사이트를 너비우선으로 스케줄링 - 해당 페이지 모든 링크를 다음 페이지를 처리하기 전에 전부 검사. (깊이 우선은 각 경로마다 모든 일을 수행한 다음. 다음 경로를 처리- 둘 중 필요한 것을 선택하면 됨)
- 스로틀링: 일정시간 가져오는 페이지의 숫자를 제한.
- URL 크기 제한 : URL길이에 제한을 두어, 순환을 중단.
- URL/사이트 블랙리스트 : 순환이 있는 사이트, URL목록을 만듦
- 패턴 발견 : 심볼릭 링크와 같은 설정은 패턴을 따름. 이 패턴이 있는 RUL을 거절함.
- 콘텐츠 지문 : 로봇은 페이지의 콘텐츠에서 몇 바이트를 얻어 체크섬을 계산. 체크섬이란 그 페이지 내용의 간략한 표현임. 이전에 봤던 체크섬을 가져오면 해당 링크는 크롤링하지 않음.
- 모니터링
- 